{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Entry Point [tensor2tensor.envs.tic_tac_toe_env:TicTacToeEnv] registered with id [T2TEnv-TicTacToeEnv-v0]\n"
     ]
    }
   ],
   "source": [
    "# Imports we need.\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.utils import t2t_model\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import metrics\n",
    "\n",
    "# Enable TF Eager execution\n",
    "tfe = tf.contrib.eager\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "# Other setup\n",
    "Modes = tf.estimator.ModeKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Problem is a dataset together with some fixed pre-processing.\n",
    "# It could be a translation dataset with a specific tokenization,\n",
    "# or an image dataset with a specific resolution.\n",
    "#\n",
    "# There are many problems available in Tensor2Tensor\n",
    "#problems.available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Generating case 0.\n",
      "INFO:tensorflow:Generated 100000 Examples\n",
      "INFO:tensorflow:Generating case 0.\n",
      "INFO:tensorflow:Generated 10000 Examples\n"
     ]
    }
   ],
   "source": [
    "data_dir='../data/algorithmic_count100'\n",
    "tmp_dir='../tmp/algorithmic_count100'\n",
    "\n",
    "# Fetch the problem\n",
    "problem = problems.problem(\"algorithmic_count100\")\n",
    "# The generate_data method of a problem will download data and process it into\n",
    "# a standard format ready for training and evaluation.\n",
    "problem.generate_data(data_dir, tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the encoders from the problem\n",
    "encoders = problem.feature_encoders(data_dir)\n",
    "\n",
    "# Setup helper functions for encoding and decoding\n",
    "def encode(input_str, output_str=None, mode=\"inputs\"):\n",
    "  \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  inputs = encoders[mode].encode(input_str) + [1]  # add EOS id\n",
    "  batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n",
    "  return {\"inputs\": batch_inputs}\n",
    "\n",
    "def decode(integers, mode=\"inputs\"):\n",
    "  \"\"\"List of ints to str\"\"\"\n",
    "#  integers = list(np.squeeze(integers))\n",
    "#  if 1 in integers:\n",
    "#    integers = integers[:integers.index(1)]\n",
    "  return encoders[mode].decode(np.squeeze(integers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from ../data/algorithmic_count100/algorithmic_count100-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 1\n",
      "input: ['3', '5', '44', '88', '48', '52', '66', '56', '67', '89', '46', '45', '85', '33', '16', '15', '95', '11', '37', '10', '35', '13', '88', '36', '8', '82', '51', '60', '96', '46', '23', '9', '39', '87', '65', '<EOS>']\n",
      "Label: ID_32\n"
     ]
    }
   ],
   "source": [
    "example = tfe.Iterator(problem.dataset(Modes.EVAL, data_dir)).next()\n",
    "example_inputs = example[\"inputs\"]\n",
    "example_label = example[\"targets\"]\n",
    "\n",
    "print(\"input:\", decode(example_inputs, mode=\"inputs\").split())\n",
    "print(\"Label: %s\" %  encoders[\"targets\"].decode(example_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID_0',\n",
       " 'ID_1',\n",
       " 'ID_2',\n",
       " 'ID_3',\n",
       " 'ID_4',\n",
       " 'ID_5',\n",
       " 'ID_6',\n",
       " 'ID_7',\n",
       " 'ID_8',\n",
       " 'ID_9',\n",
       " 'ID_10',\n",
       " 'ID_11',\n",
       " 'ID_12',\n",
       " 'ID_13',\n",
       " 'ID_14',\n",
       " 'ID_15',\n",
       " 'ID_16',\n",
       " 'ID_17',\n",
       " 'ID_18',\n",
       " 'ID_19',\n",
       " 'ID_20',\n",
       " 'ID_21',\n",
       " 'ID_22',\n",
       " 'ID_23',\n",
       " 'ID_24',\n",
       " 'ID_25',\n",
       " 'ID_26',\n",
       " 'ID_27',\n",
       " 'ID_28',\n",
       " 'ID_29',\n",
       " 'ID_30',\n",
       " 'ID_31',\n",
       " 'ID_32',\n",
       " 'ID_33',\n",
       " 'ID_34',\n",
       " 'ID_35',\n",
       " 'ID_36',\n",
       " 'ID_37',\n",
       " 'ID_38',\n",
       " 'ID_39',\n",
       " 'ID_40',\n",
       " 'ID_41',\n",
       " 'ID_42',\n",
       " 'ID_43',\n",
       " 'ID_44',\n",
       " 'ID_45',\n",
       " 'ID_46',\n",
       " 'ID_47',\n",
       " 'ID_48',\n",
       " 'ID_49',\n",
       " 'ID_50',\n",
       " 'ID_51',\n",
       " 'ID_52',\n",
       " 'ID_53',\n",
       " 'ID_54',\n",
       " 'ID_55',\n",
       " 'ID_56',\n",
       " 'ID_57',\n",
       " 'ID_58',\n",
       " 'ID_59',\n",
       " 'ID_60',\n",
       " 'ID_61',\n",
       " 'ID_62',\n",
       " 'ID_63',\n",
       " 'ID_64',\n",
       " 'ID_65',\n",
       " 'ID_66',\n",
       " 'ID_67',\n",
       " 'ID_68',\n",
       " 'ID_69',\n",
       " 'ID_70',\n",
       " 'ID_71',\n",
       " 'ID_72',\n",
       " 'ID_73',\n",
       " 'ID_74',\n",
       " 'ID_75',\n",
       " 'ID_76',\n",
       " 'ID_77',\n",
       " 'ID_78',\n",
       " 'ID_79',\n",
       " 'ID_80',\n",
       " 'ID_81',\n",
       " 'ID_82',\n",
       " 'ID_83',\n",
       " 'ID_84',\n",
       " 'ID_85',\n",
       " 'ID_86',\n",
       " 'ID_87',\n",
       " 'ID_88',\n",
       " 'ID_89',\n",
       " 'ID_90',\n",
       " 'ID_91',\n",
       " 'ID_92',\n",
       " 'ID_93',\n",
       " 'ID_94',\n",
       " 'ID_95',\n",
       " 'ID_96',\n",
       " 'ID_97',\n",
       " 'ID_98',\n",
       " 'ID_99']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders[\"targets\"]._class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from ../data/algorithmic_count100/algorithmic_count100-train*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 10\n",
      "INFO:tensorflow:Reading data files from ../data/algorithmic_count100/algorithmic_count100-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 1\n"
     ]
    }
   ],
   "source": [
    "number_of_examples = 1000\n",
    "\n",
    "dataset = problem.dataset(Modes.TRAIN, data_dir)\n",
    "dataset = dataset.padded_batch(batch_size=number_of_examples, padded_shapes={'inputs':[None], 'targets':[1], 'batch_prediction_key':[None]})\n",
    "\n",
    "batched_examples = tfe.Iterator(dataset).next()\n",
    "\n",
    "\n",
    "eval_dataset = problem.dataset(Modes.EVAL, data_dir)\n",
    "eval_dataset = eval_dataset.padded_batch(batch_size=number_of_examples, padded_shapes={'inputs':[None], 'targets':[1], 'batch_prediction_key':[None]})\n",
    "\n",
    "eval_batched_examples = tfe.Iterator(eval_dataset).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 46  46  89  41  46   3  48   1   0   0   0   0   0   0   0   0   0  79\n",
      "  35  69  82  80  57  80  69  15  79  80  21  50  50  79  35  82  15  50\n",
      "  69  69  52  15 100], shape=(41,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(eval_batched_examples['inputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711\n",
      "input: ['44', '44', '87', '39', '44', '1', '46', '<EOS>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '77', '33', '67', '80', '78', '55', '78', '67', '13', '77', '78', '19', '48', '48', '77', '33', '80', '13', '48', '67', '67', '50', '13', '98']\n",
      "Label: ID_0\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(len(eval_batched_examples['inputs']))\n",
    "print(i)\n",
    "print(\"input:\", decode(eval_batched_examples['inputs'][i], mode=\"inputs\").split())\n",
    "print(\"Label: %s\" %  encoders[\"targets\"].decode(eval_batched_examples['targets'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_from_examples(batched_examples):\n",
    "    lengthes = []\n",
    "    targets = []\n",
    "    targets_ratio = []\n",
    "    for example_inputs, example_targets in zip(batched_examples['inputs'], batched_examples['targets']):\n",
    "        np_example_inputs = example_inputs.numpy()\n",
    "        np_example_targets = example_targets.numpy()[0]\n",
    "        if 0 in list(np_example_inputs):\n",
    "            length = list(np_example_inputs).index(0)\n",
    "        else:\n",
    "            length = len(np_example_inputs)\n",
    "        lengthes.append(length)\n",
    "        targets.append(np_example_targets)\n",
    "        targets_ratio.append(float(np_example_targets)/length)\n",
    "    return lengthes, targets, targets_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-5081d139d032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlengthes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stats_from_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengthes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-ce4f460bd1df>\u001b[0m in \u001b[0;36mget_stats_from_examples\u001b[0;34m(batched_examples)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlengthes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_example_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtargets_ratio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_example_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlengthes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "lengthes, targets, targets_ratio = get_stats_from_examples(batched_examples)\n",
    "\n",
    "n, bins, patches = plt.hist(lengthes, 10, density=True, facecolor='g', alpha=0.75)\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(targets, 10, density=True, facecolor='g', alpha=0.75)\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(targets_ratio, 10, density=True, facecolor='g', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "lengthes, targets, targets_ratio = get_stats_from_examples(eval_batched_examples)\n",
    "\n",
    "n, bins, patches = plt.hist(lengthes, 20, density=True, facecolor='g', alpha=0.75)\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(targets, 20, density=True, facecolor='g', alpha=0.75)\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(targets_ratio, 20, density=True, facecolor='g', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsetting shared_embedding_and_softmax_weights.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'train'\n"
     ]
    }
   ],
   "source": [
    "#@title Create the model\n",
    "# Create hparams and the model\n",
    "model_name = \"bottomup_transformer_encoder\"\n",
    "hparams_set = \"transformer_tiny_tall\"\n",
    "\n",
    "hparams = trainer_lib.create_hparams(hparams_set, data_dir=data_dir, problem_name=\"algorithmic_count10\")\n",
    "#hparams.num_heads = 1\n",
    "# NOTE: Only create the model once when restoring from a checkpoint; it's a\n",
    "# Layer and so subsequent instantiations will have different variable scopes\n",
    "# that will not match the checkpoint.\n",
    "model = registry.model(model_name)(hparams, Modes.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from ../data/algorithmic_count10/algorithmic_count10-train*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 10\n"
     ]
    }
   ],
   "source": [
    "# Prepare for the training loop\n",
    "\n",
    "# In Eager mode, opt.minimize must be passed a loss function wrapped with\n",
    "# implicit_value_and_gradients\n",
    "@tfe.implicit_value_and_gradients\n",
    "def loss_fn(features):\n",
    "  _, losses = model(features)\n",
    "  return losses[\"training\"]\n",
    "\n",
    "# Setup the training data\n",
    "BATCH_SIZE = 128\n",
    "train_dataset = problem.dataset(Modes.TRAIN, data_dir)\n",
    "train_dataset = train_dataset.padded_batch(batch_size=BATCH_SIZE, padded_shapes={'inputs':[None], 'targets':[1], 'batch_prediction_key':[None]})\n",
    "train_dataset = train_dataset.repeat(None)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/samira/Codes/tensor2tensor/tensor2tensor/utils/t2t_model.py:1368: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_12_128.bottom\n",
      "INFO:tensorflow:Transforming feature 'targets' with class_label_modality_10_128.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "WARNING:tensorflow:From /home/samira/Codes/tensor2tensor/tensor2tensor/layers/common_attention.py:856: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Transforming body output with class_label_modality_10_128.top\n",
      "WARNING:tensorflow:From /home/samira/Codes/tensor2tensor/tensor2tensor/layers/modalities.py:946: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "Step: 0, Loss: 1.564\n",
      "Step: 50, Loss: 1.464\n",
      "Step: 100, Loss: 1.148\n",
      "Step: 150, Loss: 1.122\n",
      "Step: 200, Loss: 1.267\n",
      "Step: 250, Loss: 1.506\n",
      "Step: 300, Loss: 1.403\n",
      "Step: 350, Loss: 1.300\n",
      "Step: 400, Loss: 1.428\n",
      "Step: 450, Loss: 1.484\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1a1fb01788d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Make it 4D.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myt2t/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgrad_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mthis_tape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_new_tape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m       \u001b[0mend_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mend_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         raise ValueError(\"Cannot differentiate a function that returns None; \"\n",
      "\u001b[0;32m<ipython-input-12-14b8e5a7a9e8>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplicit_value_and_gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myt2t/lib/python3.7/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myt2t/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/tensor2tensor/tensor2tensor/utils/t2t_model.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    322\u001b[0m       \u001b[0msummarize_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_datashards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0msharded_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shard_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0msharded_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fn_sharded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharded_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharded_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mconcat_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/tensor2tensor/tensor2tensor/utils/t2t_model.py\u001b[0m in \u001b[0;36mmodel_fn_sharded\u001b[0;34m(self, sharded_features)\u001b[0m\n\u001b[1;32m    399\u001b[0m           \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loss_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m       \u001b[0msharded_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharded_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatashard_to_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m       sharded_logits, sharded_losses = dp(\n\u001b[1;32m    403\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_scheduled_sampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/tensor2tensor/tensor2tensor/utils/expert_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_devices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mDEFAULT_DEV_STRING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_devices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m               \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmy_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmy_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmy_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmy_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/tensor2tensor/tensor2tensor/utils/t2t_model.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody_vs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mlog_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building model body\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mbody_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m       \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_body_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/tensor2tensor/tensor2tensor/models/transformer.py\u001b[0m in \u001b[0;36mbody\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m   1281\u001b[0m       \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m       \u001b[0mnonpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_to_nonpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m       save_weights_to=self.attention_weights)\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoder_output_presence\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output_presence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/tensor2tensor/tensor2tensor/layers/transformer_layers.py\u001b[0m in \u001b[0;36mtransformer_bottomup_encoder\u001b[0;34m(encoder_input, encoder_self_attention_bias, hparams, name, nonpadding, save_weights_to, make_image_summary, losses, attn_bias_for_padding)\u001b[0m\n\u001b[1;32m    329\u001b[0m               \u001b[0mactivation_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"activation_dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m               \u001b[0mweight_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weight_dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m               hard_attention_k=hparams.get(\"hard_attention_k\", 0))\n\u001b[0m\u001b[1;32m    332\u001b[0m           \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ffn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/tensor2tensor/tensor2tensor/layers/common_attention.py\u001b[0m in \u001b[0;36mmultihead_attention\u001b[0;34m(query_antecedent, memory_antecedent, bias, total_key_depth, total_value_depth, output_depth, num_heads, dropout_rate, attention_type, presence_k, presence_q, max_relative_position, heads_share_relative_embedding, add_relative_to_values, image_shapes, block_length, block_width, q_filter_width, kv_filter_width, q_padding, kv_padding, cache, gap_size, num_memory_blocks, name, save_weights_to, make_image_summary, dropout_broadcast_dims, vars_3d, layer_collection, recurrent_memory, chunk_number, hard_attention_k, max_area_width, max_area_height, memory_height, area_key_mode, area_value_mode, training, **kwargs)\u001b[0m\n\u001b[1;32m   4370\u001b[0m           \u001b[0mmake_image_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_image_summary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4371\u001b[0m           \u001b[0mdropout_broadcast_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_broadcast_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4372\u001b[0;31m           activation_dtype=kwargs.get(\"activation_dtype\"))\n\u001b[0m\u001b[1;32m   4373\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mattention_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"dot_product_relative\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m       x = dot_product_attention_relative(\n",
      "\u001b[0;32m~/Codes/tensor2tensor/tensor2tensor/layers/common_attention.py\u001b[0m in \u001b[0;36mbottom_up_dot_product_attention\u001b[0;34m(q, k, v, bias, presence_q, presence_k, dropout_rate, image_shapes, name, make_image_summary, save_weights_to, dropout_broadcast_dims, activation_dtype, weight_dtype, assignment_softmax_temp, transform_presence_logits, presence_calc_mode, presence_softmax_temp, scale_factor)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0;31m# dotproduct: [length_q, embedding]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_assignment_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_q_presence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m def _generate_relative_positions_matrix(length_q, length_k,\n",
      "\u001b[0;32m~/anaconda3/envs/myt2t/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0madjoint_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m       return gen_math_ops.batch_mat_mul(\n\u001b[0;32m-> 2417\u001b[0;31m           a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n\u001b[0m\u001b[1;32m   2418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0;31m# Neither matmul nor sparse_matmul support adjoint, so we conjugate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myt2t/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mbatch_mat_mul\u001b[0;34m(x, y, adj_x, adj_y, name)\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BatchMatMul\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"adj_x\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"adj_y\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m         adj_y)\n\u001b[0m\u001b[1;32m   1402\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "NUM_STEPS = 1000\n",
    "\n",
    "for count, example in enumerate(tfe.Iterator(train_dataset)):\n",
    "  example[\"targets\"] = tf.reshape(example[\"targets\"], [BATCH_SIZE, 1, 1, 1])  # Make it 4D.\n",
    "  loss, gv = loss_fn(example)\n",
    "  optimizer.apply_gradients(gv)\n",
    "\n",
    "  if count % 50 == 0:\n",
    "    print(\"Step: %d, Loss: %.3f\" % (count, loss.numpy()))\n",
    "  if count >= NUM_STEPS:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Reading data files from ../data/algorithmic_count10/algorithmic_count10-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 1\n",
      "accuracy: 0.18\n",
      "accuracy_top5: 0.60\n"
     ]
    }
   ],
   "source": [
    "model.set_mode(Modes.EVAL)\n",
    "eval_dataset = problem.dataset(Modes.EVAL, data_dir)\n",
    "\n",
    "# Create eval metric accumulators for accuracy (ACC) and accuracy in\n",
    "# top 5 (ACC_TOP5)\n",
    "metrics_accum, metrics_result = metrics.create_eager_metrics(\n",
    "    [metrics.Metrics.ACC, metrics.Metrics.ACC_TOP5])\n",
    "\n",
    "for count, example in enumerate(tfe.Iterator(eval_dataset)):\n",
    "  if count >= 200:\n",
    "    break\n",
    "\n",
    "  \n",
    "  # Make the inputs and targets 4D\n",
    "  example[\"targets\"] = tf.reshape(example[\"targets\"], [1, 1, 1, 1])\n",
    "  example[\"inputs\"] = tf.reshape(example[\"inputs\"], [1, 1, -1, 1])\n",
    "\n",
    "  # Call the model\n",
    "  predictions, _ = model(example)\n",
    "\n",
    "  # Compute and accumulate metrics\n",
    "  metrics_accum(predictions, example[\"targets\"])\n",
    "\n",
    "# Print out the averaged metric values on the eval data\n",
    "for name, val in metrics_result().items():\n",
    "  print(\"%s: %.2f\" % (name, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the inputs and targets 4D\n",
    "\n",
    "batched_examples[\"targets\"] = tf.reshape(batched_examples[\"targets\"], [number_of_examples, 1, 1, 1])\n",
    "batched_examples[\"inputs\"] = tf.reshape(batched_examples[\"inputs\"], [number_of_examples, 1, -1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772\n",
      "[ 5  4  6 10  3  9 11  8  7  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[10]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(len(batched_examples['inputs']))\n",
    "print(i)\n",
    "print(batched_examples['inputs'][i].numpy())\n",
    "print(batched_examples['targets'][i].numpy())\n",
    "predictions, _ = model(batched_examples)\n",
    "print(np.argmax(predictions[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
